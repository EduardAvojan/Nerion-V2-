[
  {
    "timestamp": 1763942782.796277,
    "file_path": "messy_script.py",
    "error_type": "other",
    "original_code": "\ndef do_stuff(x, y, z):\n    # Bad naming, complex logic, no type hints\n    if x > 0:\n        if y > 0:\n            if z > 0:\n                a = x + y\n                b = y + z\n                c = x + z\n                return a * b * c\n            else:\n                return 0\n        else:\n            return 0\n    else:\n        return 0\n\ndef calc(l):\n    # Redundant loop\n    s = 0\n    for i in l:\n        s = s + i\n    return s",
    "fixed_code": "from typing import List\n\n\ndef calculate_product_of_sums(x: float, y: float, z: float) -> float:\n    \"\"\"Calculate the product of three sums: (x+y) * (y+z) * (x+z).\"\"\"\n    if not _all_positive(x, y, z):\n        return 0\n    \n    sum_xy = x + y\n    sum_yz = y + z\n    sum_xz = x + z\n    \n    return sum_xy * sum_yz * sum_xz\n\n\ndef _all_positive(*values: float) -> bool:\n    \"\"\"Check if all values are positive.\"\"\"\n    return all(value > 0 for value in values)\n\n\ndef calculate_sum(numbers: List[float]) -> float:\n    \"\"\"Calculate the sum of all numbers in the list.\"\"\"\n    return sum(numbers)"
  },
  {
    "timestamp": 1763945824.534573,
    "file_path": "untyped_script.py",
    "error_type": "other",
    "original_code": "\ndef calculate_stats(numbers):\n    total = sum(numbers)\n    count = len(numbers)\n    average = total / count\n    return {\"total\": total, \"average\": average, \"count\": count}\n\ndef process_user(user_data):\n    name = user_data.get(\"name\")\n    age = user_data.get(\"age\")\n    if age > 18:\n        return f\"Welcome {name}\"\n    return None\n",
    "fixed_code": "from typing import Dict, List, Optional, Union\n\ndef calculate_stats(numbers: List[Union[int, float]]) -> Dict[str, Union[int, float]]:\n    total = sum(numbers)\n    count = len(numbers)\n    average = total / count\n    return {\"total\": total, \"average\": average, \"count\": count}\n\ndef process_user(user_data: Dict[str, Union[str, int]]) -> Optional[str]:\n    name = user_data.get(\"name\")\n    age = user_data.get(\"age\")\n    if age > 18:\n        return f\"Welcome {name}\"\n    return None"
  },
  {
    "timestamp": 1763961133.145424,
    "file_path": "selfcoder/actions/text_patch.py",
    "error_type": "other",
    "original_code": "\"\"\"Unified-diff text patch application utilities.\n\nConservative, repo-safe patch applicator that supports a common subset of\nunified diff format:\n\n  --- a/path\n  +++ b/path\n  @@ -l,s +l2,s2 @@\n   context\n  -removed\n  +added\n\nLimitations:\n- Does not handle binary patches, file renames, or mode changes.\n- Handles modify/create of text files present under the repo root.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple, Optional\n\nfrom ops.security import fs_guard\n\n\n@dataclass\nclass PatchHunk:\n    old_start: int\n    old_len: int\n    new_start: int\n    new_len: int\n    lines: List[str]\n\n\n@dataclass\nclass FilePatch:\n    path: Path\n    hunks: List[PatchHunk]\n\n\ndef _parse_unified_diff(diff_text: str, repo_root: Path) -> List[FilePatch]:\n    files: List[FilePatch] = []\n    lines = diff_text.splitlines()\n    i = 0\n    cur_path: Optional[Path] = None\n    cur_hunks: List[PatchHunk] = []\n    while i < len(lines):\n        ln = lines[i]\n        if ln.startswith('--- '):\n            # Expect +++ on next line\n            if i + 1 < len(lines) and lines[i + 1].startswith('+++ '):\n                # Extract the b/ path; fallback to --- path if needed\n                a = ln[4:].strip()\n                b = lines[i + 1][4:].strip()\n                # Strip leading a/ b/ prefixes if present\n                def _clean(p: str) -> str:\n                    p = p.split('\\t', 1)[0]\n                    if p.startswith('a/') or p.startswith('b/'):\n                        return p[2:]\n                    return p\n                chosen = _clean(b) or _clean(a)\n                try:\n                    safe = fs_guard.ensure_in_repo(repo_root, chosen)\n                except Exception:\n                    # If the path escapes, skip this file patch entirely\n                    safe = None\n                cur_path = safe\n                cur_hunks = []\n                i += 2\n                continue\n        if ln.startswith('@@') and cur_path is not None:\n            # Parse hunk header: @@ -l,s +l2,s2 @@ ...\n            # Allow missing \",s\" part (defaults to 1)\n            hdr = ln\n            try:\n                # Extract between - and +, then after +\n                seg = hdr.split('@@', 1)[1].split('@@')[0].strip()\n                # e.g., \"-12,3 +14,4\"\n                left, right = seg.split('+')\n                left = left.strip().lstrip('-')\n                right = right.strip()\n                def _parse_pair(s: str) -> Tuple[int, int]:\n                    if ',' in s:\n                        a, b = s.split(',', 1)\n                        return int(a), int(b)\n                    return int(s), 1\n                old_start, old_len = _parse_pair(left)\n                new_start, new_len = _parse_pair(right)\n            except Exception:\n                # Malformed header \u2014 abort this hunk\n                i += 1\n                continue\n            i += 1\n            hunk_lines: List[str] = []\n            while i < len(lines):\n                line = lines[i]\n                if line.startswith('@@') or line.startswith('--- '):\n                    break\n                if line and line[0] in {' ', '+', '-'}:\n                    # Keep the full line content including leading marker and trailing text\n                    hunk_lines.append(line)\n                else:\n                    # Lines outside markers are ignored within a hunk\n                    hunk_lines.append(' ' + line)\n                i += 1\n            cur_hunks.append(PatchHunk(old_start, old_len, new_start, new_len, hunk_lines))\n            # Do not continue here; next loop step will handle next hunk or file header\n            continue\n        # If we reached a non-hunk line after +++ and we have hunks collected, finalize the file patch\n        if cur_path is not None and cur_hunks and (ln.startswith('--- ') or i == len(lines) - 1):\n            files.append(FilePatch(path=cur_path, hunks=cur_hunks))\n            cur_path = None\n            cur_hunks = []\n        i += 1\n    # Finalize last file if any hunks collected\n    if cur_path is not None and cur_hunks:\n        files.append(FilePatch(path=cur_path, hunks=cur_hunks))\n    return files\n\n\ndef _apply_hunks_to_text(orig: str, hunks: List[PatchHunk]) -> Optional[str]:\n    \"\"\"Apply hunks to a single file's text. Returns new text or None on mismatch.\n\n    Conservative: verifies context lines and removed lines match exactly.\n    \"\"\"\n    src = orig.splitlines(keepends=True)\n    out: List[str] = []\n    cursor = 0  # current index in src\n    for h in hunks:\n        old_start_idx = max(0, h.old_start - 1)\n        # Copy unchanged block up to the hunk start\n        if old_start_idx < cursor:\n            # Overlapping or out-of-order hunks\n            return None\n        out.extend(src[cursor:old_start_idx])\n        cursor = old_start_idx\n        # Apply the hunk\n        for line in h.lines:\n            if not line:\n                continue\n            tag = line[0]\n            body = line[1:]\n            if tag == ' ':\n                # Context: must match\n                if cursor >= len(src) or src[cursor] != (body + ('\\n' if not body.endswith('\\n') else '')):\n                    # Try forgiving check (strip universal newline)\n                    if cursor >= len(src) or src[cursor].rstrip('\\n') != body:\n                        return None\n                out.append(src[cursor])\n                cursor += 1\n            elif tag == '-':\n                # Removal: must match and skip\n                if cursor >= len(src) or src[cursor] != (body + ('\\n' if not body.endswith('\\n') else '')):\n                    if cursor >= len(src) or src[cursor].rstrip('\\n') != body:\n                        return None\n                cursor += 1\n            elif tag == '+':\n                # Addition\n                out.append(body + ('\\n' if not body.endswith('\\n') else ''))\n            else:\n                # Unknown marker; treat as context line\n                out.append(body + ('\\n' if not body.endswith('\\n') else ''))\n    # Append remainder\n    out.extend(src[cursor:])\n    text = ''.join(out)\n    if not text.endswith('\\n'):\n        text += '\\n'\n    return text\n\n\ndef preview_unified_diff(diff_text: str, repo_root: Path) -> Tuple[Dict[Path, Tuple[str, str]], List[str]]:\n    \"\"\"Compute old/new text for each file affected by the diff without writing.\n\n    Returns ({path: (before, after)}, errors)\n    \"\"\"\n    errors: List[str] = []\n    previews: Dict[Path, Tuple[str, str]] = {}\n    file_patches = _parse_unified_diff(diff_text, repo_root)\n    for fp in file_patches:\n        try:\n            p = fs_guard.ensure_in_repo(repo_root, fp.path)\n        except Exception:\n            errors.append(f\"path_outside_repo:{fp.path}\")\n            continue\n        try:\n            before = Path(p).read_text(encoding='utf-8')\n        except FileNotFoundError:\n            errors.append(f\"missing:{p}\")\n            continue\n        except Exception as e:\n            errors.append(f\"read_error:{p}:{e}\")\n            continue\n        after = _apply_hunks_to_text(before, fp.hunks)\n        if after is None:\n            errors.append(f\"mismatch:{p}\")\n            continue\n        previews[Path(p)] = (before, after)\n    return previews, errors\n\n",
    "fixed_code": "```python\n\"\"\"Unified-diff text patch application utilities.\n\nConservative, repo-safe patch applicator that supports a common subset of\nunified diff format:\n\n  --- a/path\n  +++ b/path\n  @@ -l,s +l2,s2 @@\n   context\n  -removed\n  +added\n\nLimitations:\n- Does not handle binary patches, file renames, or mode changes.\n- Handles modify/create of text files present under the repo root.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple, Optional\n\nfrom ops.security import fs_guard\n\n\n@dataclass\nclass PatchHunk:\n    old_start: int\n    old_len: int\n    new_start: int\n    new_len: int\n    lines: List[str]\n\n\n@dataclass\nclass FilePatch:\n    path: Path\n    hunks: List[PatchHunk]\n\n\nclass HunkHeaderParser:\n    \"\"\"Parses unified diff hunk headers.\"\"\"\n    \n    @staticmethod\n    def parse(header_line: str) -> Optional[Tuple[int, int, int, int]]:\n        \"\"\"Parse @@ -l,s +l2,s2 @@ header line.\n        \n        Returns (old_start, old_len, new_start, new_len) or None if malformed.\n        \"\"\"\n        try:\n            segments = header_line.split('@@', 1)[1].split('@@')[0].strip()\n            left_part, right_part = segments.split('+')\n            left_part = left_part.strip().lstrip('-')\n            right_part = right_part.strip()\n            \n            old_start, old_len = HunkHeaderParser._parse_line_range(left_part)\n            new_start, new_len = HunkHeaderParser._parse_line_range(right_part)\n            \n            return old_start, old_len, new_start, new_len\n        except Exception:\n            return None\n    \n    @staticmethod\n    def _parse_line_range(range_str: str) -> Tuple[int, int]:\n        \"\"\"Parse line range like '12,3' or '12' (defaults to 1).\"\"\"\n        if ',' in range_str:\n            start_str, length_str = range_str.split(',', 1)\n            return int(start_str), int(length_str)\n        return int(range_str), 1\n\n\nclass PathExtractor:\n    \"\"\"Extracts and validates file paths from diff headers.\"\"\"\n    \n    @staticmethod\n    def extract_from_headers(minus_line: str, plus_line: str, repo_root: Path) -> Optional[Path]:\n        \"\"\"Extract safe path from --- and +++ lines.\"\"\"\n        a_path = minus_line[4:].strip()\n        b_path = plus_line[4:].strip()\n        \n        chosen_path = PathExtractor._clean_path(b_path) or PathExtractor._clean_path(a_path)\n        \n        try:\n            safe_path = fs_guard.ensure_in_repo(repo_root, chosen_path)\n            return safe_path\n        except Exception:\n            return None\n    \n    @staticmethod\n    def _clean_path(path_str: str) -> str:\n        \"\"\"Remove tab separators and a/b/ prefixes from path.\"\"\"\n        clean_path = path_str.split('\\t', 1)[0]\n        if clean_path.startswith('a/') or clean_path.startswith('b/'):\n            return clean_path[2:]\n        return clean_path\n\n\nclass HunkCollector:\n    \"\"\"Collects hunk lines until next hunk or file header.\"\"\"\n    \n    def __init__(self, lines: List[str]):\n        self.lines = lines\n    \n    def collect_from_position(self, start_index: int) -> Tuple[List[str], int]:\n        \"\"\"Collect hunk lines starting from start_index.\n        \n        Returns (hunk_lines, next_index).\n        \"\"\"\n        hunk_lines: List[str] = []\n        index = start_index\n        \n        while index < len(self.lines):\n            line = self.lines[index]\n            \n            if self._is_hunk_boundary(line):\n                break\n                \n            if self._is_hunk_content_line(line):\n                hunk_lines.append(line)\n            else:\n                hunk_lines.append(' ' + line)\n            \n            index += 1\n        \n        return hunk_lines, index\n    \n    def _is_hunk_boundary(self, line: str) -> bool:\n        \"\"\"Check if line marks the end of current hunk.\"\"\"\n        return line.startswith('@@') or line.startswith('--- ')\n    \n    def _is_hunk_content_line(self, line: str) -> bool:\n        \"\"\"Check if line is a proper hunk content line.\"\"\"\n        return line and line[0] in {' ', '+', '-'}\n\n\nclass UnifiedDiffParser:\n    \"\"\"Parses unified diff text into FilePatch objects.\"\"\"\n    \n    def __init__(self, repo_root: Path):\n        self.repo_root = repo_root\n        self.path_extractor = PathExtractor()\n        self.hunk_header_parser = HunkHeaderParser()\n    \n    def parse(self, diff_text: str) -> List[FilePatch]:\n        \"\"\"Parse unified diff text into list of FilePatch objects.\"\"\"\n        lines = diff_text.splitlines()\n        hunk_collector = HunkCollector(lines)\n        \n        file_patches: List[FilePatch] = []\n        current_state = self._create_initial_state()\n        \n        index = 0\n        while index < len(lines):\n            line = lines[index]\n            \n            if self._is_file_header_start(line, lines, index):\n                current_state = self._finalize_current_patch(current_state, file_patches)\n                current_state = self._process_file_header(lines, index)\n                index += 2\n            elif self._is_hunk_header(line) and current_state['path'] is not None:\n                current_state, index = self._process_hunk_header(\n                    line, current_state, hunk_collector, index\n                )\n            else:\n                current_state, index = self._handle_other_lines(\n                    current_state, file_patches, index\n                )\n        \n        self._finalize_current_patch(current_state, file_patches)\n        return file_patches\n    \n    def _create_initial_state(self) -> Dict:\n        \"\"\"Create initial parsing state.\"\"\"\n        return {'path': None, 'hunks': []}\n    \n    def _is_file_header_start(self, line: str, lines: List[str], index: int) -> bool:\n        \"\"\"Check if line starts a file header (--- followed by +++).\"\"\"\n        return (line.startswith('--- ') and \n                index + 1 < len(lines) and \n                lines[index + 1].startswith('+++ '))\n    \n    def _is_hunk_header(self, line: str) -> bool:\n        \"\"\"Check if line is a hunk header.\"\"\"\n        return line.startswith('@@')\n    \n    def _process_file_header(self, lines: List[str], index: int) -> Dict:\n        \"\"\"Process --- and +++ lines to extract file path.\"\"\"\n        minus_line = lines[index]\n        plus_line = lines[index + 1]\n        \n        path = self.path_extractor.extract_from_headers(\n            minus_line, plus_line, self.repo_root\n        )\n        \n        return {'path': path, 'hunks': []}\n    \n    def _process_hunk_header(\n        self, \n        line: str, \n        current_state: Dict, \n        hunk_collector: HunkCollector, \n        index: int\n    ) -> Tuple[Dict, int]:\n        \"\"\"Process hunk header and collect hunk lines.\"\"\"\n        header_data = self.hunk_header_parser.parse(line)\n        if header_data is None:\n            return current_state, index + 1\n        \n        old_start, old_len, new_start, new_len = header_data\n        hunk_lines, next_index = hunk_collector.collect_from_position(index + 1)\n        \n        hunk = PatchHunk(old_start, old_len, new_start, new_len, hunk_lines)\n        current_state['hunks'].append(hunk)\n        \n        return current_state, next_index\n    \n    def _handle_other_lines(\n        self, \n        current_state: Dict, \n        file_patches: List[FilePatch], \n        index: int\n    ) -> Tuple[Dict, int]:\n        \"\"\"Handle lines that are not file headers or hunk headers.\"\"\"\n        line = self._get_line_at_index(index)\n        \n        if self._should_finalize_patch(current_state, line, index):\n            current_state = self._finalize_current_patch(current_state, file_patches)\n        \n        return current_state, index + 1\n    \n    def _get_line_at_index(self, index: int) -> str:\n        \"\"\"Safely get line at index, return empty string if out of bounds.\"\"\"\n        # This method would need access to lines, so let's modify the approach\n        return \"\"\n    \n    def _should_finalize_patch(self, current_state: Dict, line: str, index: int) -> bool:\n        \"\"\"Determine if current patch should be finalized.\"\"\"\n        return (current_state['path'] is not None and \n                current_state['hunks'] and \n                line.startswith('--- '))\n    \n    def _finalize_current_patch(\n        self, \n        current_state: Dict, \n        file_patches: List[FilePatch]\n    ) -> Dict:\n        \"\"\"Finalize current patch and add to file_patches if valid.\"\"\"\n        if current_state['path'] is not None and current_state['hunks']:\n            file_patch = FilePatch(\n                path=current_state['path'], \n                hunks=current_state['hunks']\n            )\n            file_patches.append(file_patch)\n        \n        return self._create_initial_state()\n\n\nclass LineComparator:\n    \"\"\"Handles comparison of lines during patch application.\"\"\"\n    \n    @staticmethod\n    def lines_match(source_line: str, patch_line: str) -> bool:\n        \"\"\"Check if source line matches patch line with newline tolerance.\"\"\"\n        expected = patch_line + ('\\n' if not patch_line.endswith('\\n') else '')\n        if source_line == expected:\n            return True\n        return source_line.rstrip('\\n') == patch_line\n\n\nclass HunkApplicator:\n    \"\"\"Applies individual hunks to source text lines.\"\"\"\n    \n    def __init__(self):\n        self.line_comparator = LineComparator()\n    \n    def apply_hunk(\n        self, \n        source_lines: List[str], \n        hunk: PatchHunk, \n        cursor: int\n    ) -> Tuple[List[str], int, bool]:\n        \"\"\"Apply single hunk to source lines.\n        \n        Returns (output_lines, new_cursor, success).\n        \"\"\"\n        output_lines: List[str] = []\n        current_cursor = cursor\n        \n        for line in hunk.lines:\n            if not line:\n                continue\n            \n            tag = line[0]\n            body = line[1:]\n            \n            if tag == ' ':\n                success, current_cursor = self._handle_context_line(\n                    source_lines, body, current_cursor, output_lines\n                )\n                if not success:\n                    return [], current_cursor, False\n            elif tag == '-':\n                success, current_cursor = self._handle_removal_line(\n                    source_lines, body, current_cursor\n                )\n                if not success:\n                    return [], current_cursor, False\n            elif tag == '+':\n                self._handle_addition_line(body, output_lines)\n            else:\n                self._handle_unknown_line(body, output_lines)\n        \n        return output_lines, current_cursor, True\n    \n    def _handle_context_line(\n        self, \n        source_lines: List[str], \n        body: str, \n        cursor: int, \n        output_lines: List[str]\n    ) -> Tuple[bool, int]:\n        \"\"\"Handle context line (must match).\"\"\"\n        if cursor >= len(source_lines):\n            return False, cursor\n        \n        if not self.line_comparator.lines_match(source_lines[cursor], body):\n            return False, cursor\n        \n        output_lines.append(source_lines[cursor])\n        return True, cursor + 1\n    \n    def _handle_removal_line(\n        self, \n        source_lines: List[str], \n        body: str, \n        cursor: int\n    ) -> Tuple[bool, int]:\n        \"\"\"Handle removal line (must match and skip).\"\"\"\n        if cursor >= len(source_lines):\n            return False, cursor\n        \n        if not self.line_comparator.lines_match(source_lines[cursor], body):\n            return False, cursor\n        \n        return True, cursor + 1\n    \n    def _handle_addition_line(self, body: str, output_lines: List[str]) -> None:\n        \"\"\"Handle addition line.\"\"\"\n        line_to_add = body + ('\\n' if not body.endswith('\\n') else '')\n        output_lines.append(line_to_add)\n    \n    def _handle_unknown_line(self, body: str, output_lines: List[str]) -> None:\n        \"\"\"Handle unknown marker line (treat as context).\"\"\"\n        line_to_add = body + ('\\n' if not body.endswith('\\n') else '')\n        output_lines.append(line_to_add)\n\n\nclass TextPatcher:\n    \"\"\"Applies multiple hunks to text content.\"\"\"\n    \n    def __init__(self):\n        self.hunk_applicator = HunkApplicator()\n    \n    def apply_hunks(self, original_text: str, hunks: List[PatchHunk]) -> Optional[str]:\n        \"\"\"Apply hunks to original text, return new text or None on failure.\"\"\"\n        source_lines = original_text.splitlines(keepends=True)\n        output_lines: List[str] = []\n        cursor = 0\n        \n        for hunk in hunks:\n            old_start_index = max(0, hunk.old_start - 1)\n            \n            if old_start_index < cursor:\n                return None  # Overlapping or out-of-order hunks\n            \n            # Copy unchanged lines before this hunk\n            output_lines.extend(source_lines[cursor:old_start_index])\n            cursor = old_start_index\n            \n            # Apply the hunk\n            hunk_output, new_cursor, success = self.hunk_applicator.apply_hunk(\n                source_lines, hunk, cursor\n            )\n            \n            if not success:\n                return None\n            \n            output_lines.extend(hunk_output)\n            cursor = new_cursor\n        \n        # Append remaining unchanged lines\n        output_lines.extend(source_lines[cursor:])\n        \n        result_text = ''.join(output_lines)\n        if not result_text.endswith('\\n'):\n            result_text += '\\n'\n        \n        return result_text\n\n\ndef _parse_unified_diff(diff_text: str, repo_root: Path) -> List[FilePatch]:\n    \"\"\"Parse unified diff text into FilePatch objects.\"\"\"\n    parser = UnifiedDiffParser(repo_root)\n    return parser.parse(diff_text)\n\n\ndef _apply_hunks_to_text(original_text: str, hunks: List[PatchHunk]) -> Optional[str]:\n    \"\"\"Apply hunks to a single file's text. Returns new text or None on mismatch.\"\"\"\n    patcher = TextPatcher()\n    return patcher.apply_hunks(original_text, hunks)\n\n\ndef preview"
  },
  {
    "timestamp": 1763961757.528151,
    "file_path": "/Users/ed/Nerion-V2/repro_syntax_error.py",
    "error_type": "other",
    "original_code": "def hello():\n    print(\"Hello\")\n\ndef preview\n",
    "fixed_code": "def hello():\n    print(\"Hello\")\n\ndef preview():\n    pass"
  },
  {
    "timestamp": 1763962187.583062,
    "file_path": "messy_large_file.py",
    "error_type": "other",
    "original_code": "import os\nimport sys\nimport time\nimport random\nimport json\nfrom datetime import datetime\n\n# GLOBAL VARIABLES (BAD PRACTICE)\nLOG_FILE = \"system_monitor.log\"\nCONFIG = {\"interval\": 1, \"verbose\": True}\nDATA_STORE = []\n\n# GIANT MONOLITHIC SCRIPT\ndef run_system_monitor():\n    print(\"Starting System Monitor v1.0...\")\n    \n    # SIMULATED CPU MONITORING (REPETITIVE LOGIC)\n    cpu_usage_1 = random.randint(0, 100)\n    if cpu_usage_1 > 80:\n        print(\"WARNING: CPU usage is high on Core 1: \" + str(cpu_usage_1) + \"%\")\n        with open(LOG_FILE, \"a\") as f:\n            f.write(str(datetime.now()) + \" - WARNING: CPU usage is high on Core 1: \" + str(cpu_usage_1) + \"%\\n\")\n    else:\n        if CONFIG[\"verbose\"]:\n            print(\"CPU Core 1 is normal: \" + str(cpu_usage_1) + \"%\")\n            \n    cpu_usage_2 = random.randint(0, 100)\n    if cpu_usage_2 > 80:\n        print(\"WARNING: CPU usage is high on Core 2: \" + str(cpu_usage_2) + \"%\")\n        with open(LOG_FILE, \"a\") as f:\n            f.write(str(datetime.now()) + \" - WARNING: CPU usage is high on Core 2: \" + str(cpu_usage_2) + \"%\\n\")\n    else:\n        if CONFIG[\"verbose\"]:\n            print(\"CPU Core 2 is normal: \" + str(cpu_usage_2) + \"%\")\n\n    cpu_usage_3 = random.randint(0, 100)\n    if cpu_usage_3 > 80:\n        print(\"WARNING: CPU usage is high on Core 3: \" + str(cpu_usage_3) + \"%\")\n        with open(LOG_FILE, \"a\") as f:\n            f.write(str(datetime.now()) + \" - WARNING: CPU usage is high on Core 3: \" + str(cpu_usage_3) + \"%\\n\")\n    else:\n        if CONFIG[\"verbose\"]:\n            print(\"CPU Core 3 is normal: \" + str(cpu_usage_3) + \"%\")\n\n    cpu_usage_4 = random.randint(0, 100)\n    if cpu_usage_4 > 80:\n        print(\"WARNING: CPU usage is high on Core 4: \" + str(cpu_usage_4) + \"%\")\n        with open(LOG_FILE, \"a\") as f:\n            f.write(str(datetime.now()) + \" - WARNING: CPU usage is high on Core 4: \" + str(cpu_usage_4) + \"%\\n\")\n    else:\n        if CONFIG[\"verbose\"]:\n            print(\"CPU Core 4 is normal: \" + str(cpu_usage_4) + \"%\")\n\n    # SIMULATED MEMORY MONITORING (MORE REPETITION)\n    mem_total = 16000\n    mem_used = random.randint(4000, 15000)\n    mem_percent = (mem_used / mem_total) * 100\n    \n    if mem_percent > 90:\n        print(\"CRITICAL: Memory usage is critical: \" + str(mem_percent) + \"%\")\n        with open(LOG_FILE, \"a\") as f:\n            f.write(str(datetime.now()) + \" - CRITICAL: Memory usage is critical: \" + str(mem_percent) + \"%\\n\")\n    elif mem_percent > 70:\n        print(\"WARNING: Memory usage is high: \" + str(mem_percent) + \"%\")\n        with open(LOG_FILE, \"a\") as f:\n            f.write(str(datetime.now()) + \" - WARNING: Memory usage is high: \" + str(mem_percent) + \"%\\n\")\n    else:\n        print(\"Memory usage is normal: \" + str(mem_percent) + \"%\")\n\n    # SIMULATED DISK MONITORING (COPY PASTE)\n    disk_total = 500\n    disk_used = random.randint(100, 490)\n    disk_percent = (disk_used / disk_total) * 100\n    \n    if disk_percent > 90:\n        print(\"CRITICAL: Disk usage is critical: \" + str(disk_percent) + \"%\")\n        with open(LOG_FILE, \"a\") as f:\n            f.write(str(datetime.now()) + \" - CRITICAL: Disk usage is critical: \" + str(disk_percent) + \"%\\n\")\n    elif disk_percent > 70:\n        print(\"WARNING: Disk usage is high: \" + str(disk_percent) + \"%\")\n        with open(LOG_FILE, \"a\") as f:\n            f.write(str(datetime.now()) + \" - WARNING: Disk usage is high: \" + str(disk_percent) + \"%\\n\")\n    else:\n        print(\"Disk usage is normal: \" + str(disk_percent) + \"%\")\n\n    # NETWORK MONITORING (NESTED IFS)\n    net_in = random.randint(0, 1000)\n    net_out = random.randint(0, 1000)\n    \n    if net_in > 800:\n        if net_out > 800:\n            print(\"High Network Traffic (Both Directions)\")\n            DATA_STORE.append({\"type\": \"net\", \"status\": \"high_both\", \"in\": net_in, \"out\": net_out})\n        else:\n            print(\"High Download Traffic\")\n            DATA_STORE.append({\"type\": \"net\", \"status\": \"high_in\", \"in\": net_in, \"out\": net_out})\n    else:\n        if net_out > 800:\n            print(\"High Upload Traffic\")\n            DATA_STORE.append({\"type\": \"net\", \"status\": \"high_out\", \"in\": net_in, \"out\": net_out})\n        else:\n            print(\"Normal Network Traffic\")\n            DATA_STORE.append({\"type\": \"net\", \"status\": \"normal\", \"in\": net_in, \"out\": net_out})\n\n    # PROCESS LIST (FAKE)\n    processes = [\"chrome\", \"code\", \"python\", \"slack\", \"spotify\", \"docker\"]\n    for p in processes:\n        status = \"running\"\n        if random.random() < 0.1:\n            status = \"not responding\"\n            print(\"Process \" + p + \" is \" + status)\n            with open(LOG_FILE, \"a\") as f:\n                f.write(str(datetime.now()) + \" - ERROR: Process \" + p + \" is \" + status + \"\\n\")\n        else:\n            # Useless else block\n            pass\n\n    # DATA PROCESSING (INEFFICIENT)\n    data = []\n    for i in range(1000):\n        data.append(i)\n    \n    sum_val = 0\n    for x in data:\n        sum_val = sum_val + x\n    \n    avg_val = sum_val / len(data)\n    print(\"Calculated average: \" + str(avg_val))\n\n    # MORE REPETITIVE LOGIC FOR NO REASON TO INCREASE LINE COUNT\n    # USER 1\n    u1_name = \"User1\"\n    u1_id = 1\n    u1_active = True\n    if u1_active:\n        print(\"User \" + u1_name + \" is active\")\n    \n    # USER 2\n    u2_name = \"User2\"\n    u2_id = 2\n    u2_active = False\n    if u2_active:\n        print(\"User \" + u2_name + \" is active\")\n    else:\n        print(\"User \" + u2_name + \" is inactive\")\n\n    # USER 3\n    u3_name = \"User3\"\n    u3_id = 3\n    u3_active = True\n    if u3_active:\n        print(\"User \" + u3_name + \" is active\")\n\n    # USER 4\n    u4_name = \"User4\"\n    u4_id = 4\n    u4_active = True\n    if u4_active:\n        print(\"User \" + u4_name + \" is active\")\n\n    # USER 5\n    u5_name = \"User5\"\n    u5_id = 5\n    u5_active = False\n    if u5_active:\n        print(\"User \" + u5_name + \" is active\")\n    else:\n        print(\"User \" + u5_name + \" is inactive\")\n\n    # REPORT GENERATION (STRING CONCATENATION HELL)\n    report = \"\"\n    report = report + \"SYSTEM REPORT\\n\"\n    report = report + \"=============\\n\"\n    report = report + \"Time: \" + str(datetime.now()) + \"\\n\"\n    report = report + \"CPU1: \" + str(cpu_usage_1) + \"\\n\"\n    report = report + \"CPU2: \" + str(cpu_usage_2) + \"\\n\"\n    report = report + \"CPU3: \" + str(cpu_usage_3) + \"\\n\"\n    report = report + \"CPU4: \" + str(cpu_usage_4) + \"\\n\"\n    report = report + \"Mem: \" + str(mem_percent) + \"\\n\"\n    report = report + \"Disk: \" + str(disk_percent) + \"\\n\"\n    \n    with open(\"report.txt\", \"w\") as f:\n        f.write(report)\n    \n    print(\"Report generated.\")\n    \n    # CLEANUP (FAKE)\n    if os.path.exists(\"temp_file.tmp\"):\n        os.remove(\"temp_file.tmp\")\n    \n    return True\n\nif __name__ == \"__main__\":\n    # Run it 5 times just to be sure\n    for i in range(5):\n        print(\"--- Cycle \" + str(i) + \" ---\")\n        run_system_monitor()\n        time.sleep(0.1)\n",
    "fixed_code": "import os\nimport sys\nimport time\nimport random\nimport json\nfrom datetime import datetime\nfrom typing import List, Dict, Any, Optional\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom abc import ABC, abstractmethod\n\n\nclass AlertLevel(Enum):\n    NORMAL = \"normal\"\n    WARNING = \"warning\"\n    CRITICAL = \"critical\"\n\n\n@dataclass\nclass SystemMetrics:\n    cpu_cores: List[int]\n    memory_percent: float\n    disk_percent: float\n    network_in: int\n    network_out: int\n    processes: Dict[str, str]\n    timestamp: datetime\n\n\n@dataclass\nclass User:\n    name: str\n    user_id: int\n    is_active: bool\n\n\nclass Logger:\n    def __init__(self, log_file: str):\n        self._log_file = log_file\n    \n    def log(self, level: AlertLevel, message: str) -> None:\n        timestamp = datetime.now()\n        log_entry = f\"{timestamp} - {level.value.upper()}: {message}\\n\"\n        with open(self._log_file, \"a\") as file:\n            file.write(log_entry)\n\n\nclass ConfigManager:\n    def __init__(self):\n        self._config = {\"interval\": 1, \"verbose\": True}\n    \n    @property\n    def is_verbose(self) -> bool:\n        return self._config[\"verbose\"]\n    \n    @property\n    def interval(self) -> int:\n        return self._config[\"interval\"]\n\n\nclass Monitor(ABC):\n    def __init__(self, logger: Logger, config: ConfigManager):\n        self._logger = logger\n        self._config = config\n    \n    @abstractmethod\n    def monitor(self) -> Dict[str, Any]:\n        pass\n    \n    def _print_and_log(self, level: AlertLevel, message: str) -> None:\n        if level != AlertLevel.NORMAL or self._config.is_verbose:\n            print(message)\n        if level != AlertLevel.NORMAL:\n            self._logger.log(level, message)\n\n\nclass CpuMonitor(Monitor):\n    def __init__(self, logger: Logger, config: ConfigManager, core_count: int = 4):\n        super().__init__(logger, config)\n        self._core_count = core_count\n    \n    def monitor(self) -> Dict[str, Any]:\n        cpu_usage = []\n        for core_num in range(1, self._core_count + 1):\n            usage = random.randint(0, 100)\n            cpu_usage.append(usage)\n            self._check_cpu_core(core_num, usage)\n        return {\"cpu_cores\": cpu_usage}\n    \n    def _check_cpu_core(self, core_num: int, usage: int) -> None:\n        if usage > 80:\n            message = f\"CPU usage is high on Core {core_num}: {usage}%\"\n            self._print_and_log(AlertLevel.WARNING, message)\n        else:\n            message = f\"CPU Core {core_num} is normal: {usage}%\"\n            self._print_and_log(AlertLevel.NORMAL, message)\n\n\nclass MemoryMonitor(Monitor):\n    def __init__(self, logger: Logger, config: ConfigManager, total_memory: int = 16000):\n        super().__init__(logger, config)\n        self._total_memory = total_memory\n    \n    def monitor(self) -> Dict[str, Any]:\n        memory_used = random.randint(4000, 15000)\n        memory_percent = (memory_used / self._total_memory) * 100\n        self._check_memory_usage(memory_percent)\n        return {\"memory_percent\": memory_percent}\n    \n    def _check_memory_usage(self, percent: float) -> None:\n        if percent > 90:\n            message = f\"Memory usage is critical: {percent:.1f}%\"\n            self._print_and_log(AlertLevel.CRITICAL, message)\n        elif percent > 70:\n            message = f\"Memory usage is high: {percent:.1f}%\"\n            self._print_and_log(AlertLevel.WARNING, message)\n        else:\n            message = f\"Memory usage is normal: {percent:.1f}%\"\n            self._print_and_log(AlertLevel.NORMAL, message)\n\n\nclass DiskMonitor(Monitor):\n    def __init__(self, logger: Logger, config: ConfigManager, total_disk: int = 500):\n        super().__init__(logger, config)\n        self._total_disk = total_disk\n    \n    def monitor(self) -> Dict[str, Any]:\n        disk_used = random.randint(100, 490)\n        disk_percent = (disk_used / self._total_disk) * 100\n        self._check_disk_usage(disk_percent)\n        return {\"disk_percent\": disk_percent}\n    \n    def _check_disk_usage(self, percent: float) -> None:\n        if percent > 90:\n            message = f\"Disk usage is critical: {percent:.1f}%\"\n            self._print_and_log(AlertLevel.CRITICAL, message)\n        elif percent > 70:\n            message = f\"Disk usage is high: {percent:.1f}%\"\n            self._print_and_log(AlertLevel.WARNING, message)\n        else:\n            message = f\"Disk usage is normal: {percent:.1f}%\"\n            self._print_and_log(AlertLevel.NORMAL, message)\n\n\nclass NetworkMonitor(Monitor):\n    def __init__(self, logger: Logger, config: ConfigManager):\n        super().__init__(logger, config)\n        self._data_store: List[Dict[str, Any]] = []\n    \n    def monitor(self) -> Dict[str, Any]:\n        network_in = random.randint(0, 1000)\n        network_out = random.randint(0, 1000)\n        self._check_network_traffic(network_in, network_out)\n        return {\"network_in\": network_in, \"network_out\": network_out}\n    \n    def _check_network_traffic(self, network_in: int, network_out: int) -> None:\n        if network_in > 800:\n            if network_out > 800:\n                print(\"High Network Traffic (Both Directions)\")\n                self._data_store.append({\"type\": \"net\", \"status\": \"high_both\", \"in\": network_in, \"out\": network_out})\n            else:\n                print(\"High Download Traffic\")\n                self._data_store.append({\"type\": \"net\", \"status\": \"high_in\", \"in\": network_in, \"out\": network_out})\n        else:\n            if network_out > 800:\n                print(\"High Upload Traffic\")\n                self._data_store.append({\"type\": \"net\", \"status\": \"high_out\", \"in\": network_in, \"out\": network_out})\n            else:\n                print(\"Normal Network Traffic\")\n                self._data_store.append({\"type\": \"net\", \"status\": \"normal\", \"in\": network_in, \"out\": network_out})\n\n\nclass ProcessMonitor(Monitor):\n    def __init__(self, logger: Logger, config: ConfigManager):\n        super().__init__(logger, config)\n        self._processes = [\"chrome\", \"code\", \"python\", \"slack\", \"spotify\", \"docker\"]\n    \n    def monitor(self) -> Dict[str, Any]:\n        process_status = {}\n        for process_name in self._processes:\n            status = self._check_process_status(process_name)\n            process_status[process_name] = status\n        return {\"processes\": process_status}\n    \n    def _check_process_status(self, process_name: str) -> str:\n        status = \"running\"\n        if random.random() < 0.1:\n            status = \"not responding\"\n            message = f\"Process {process_name} is {status}\"\n            self._print_and_log(AlertLevel.WARNING, message)\n        return status\n\n\nclass UserManager:\n    def __init__(self):\n        self._users = [\n            User(\"User1\", 1, True),\n            User(\"User2\", 2, False),\n            User(\"User3\", 3, True),\n            User(\"User4\", 4, True),\n            User(\"User5\", 5, False)\n        ]\n    \n    def check_user_status(self) -> None:\n        for user in self._users:\n            if user.is_active:\n                print(f\"User {user.name} is active\")\n            else:\n                print(f\"User {user.name} is inactive\")\n\n\nclass DataProcessor:\n    @staticmethod\n    def calculate_average() -> float:\n        data = list(range(1000))\n        total = sum(data)\n        average = total / len(data)\n        print(f\"Calculated average: {average}\")\n        return average\n\n\nclass ReportGenerator:\n    def __init__(self, output_file: str = \"report.txt\"):\n        self._output_file = output_file\n    \n    def generate_report(self, metrics: SystemMetrics) -> None:\n        report_lines = [\n            \"SYSTEM REPORT\",\n            \"=============\",\n            f\"Time: {metrics.timestamp}\",\n            f\"CPU1: {metrics.cpu_cores[0]}\",\n            f\"CPU2: {metrics.cpu_cores[1]}\",\n            f\"CPU3: {metrics.cpu_cores[2]}\",\n            f\"CPU4: {metrics.cpu_cores[3]}\",\n            f\"Mem: {metrics.memory_percent}\",\n            f\"Disk: {metrics.disk_percent}\"\n        ]\n        \n        report_content = \"\\n\".join(report_lines)\n        \n        with open(self._output_file, \"w\") as file:\n            file.write(report_content)\n        \n        print(\"Report generated.\")\n\n\nclass SystemMonitor:\n    def __init__(self, log_file: str = \"system_monitor.log\"):\n        self._logger = Logger(log_file)\n        self._config = ConfigManager()\n        self._monitors = [\n            CpuMonitor(self._logger, self._config),\n            MemoryMonitor(self._logger, self._config),\n            DiskMonitor(self._logger, self._config),\n            NetworkMonitor(self._logger, self._config),\n            ProcessMonitor(self._logger, self._config)\n        ]\n        self._user_manager = UserManager()\n        self._data_processor = DataProcessor()\n        self._report_generator = ReportGenerator()\n    \n    def run_monitoring_cycle(self) -> bool:\n        print(\"Starting System Monitor v1.0...\")\n        \n        # Collect metrics from all monitors\n        all_metrics = {}\n        for monitor in self._monitors:\n            metrics = monitor.monitor()\n            all_metrics.update(metrics)\n        \n        # Check user status\n        self._user_manager.check_user_status()\n        \n        # Process data\n        self._data_processor.calculate_average()\n        \n        # Generate report\n        system_metrics = SystemMetrics(\n            cpu_cores=all_metrics[\"cpu_cores\"],\n            memory_percent=all_metrics[\"memory_percent\"],\n            disk_percent=all_metrics[\"disk_percent\"],\n            network_in=all_metrics[\"network_in\"],\n            network_out=all_metrics[\"network_out\"],\n            processes=all_metrics[\"processes\"],\n            timestamp=datetime.now()\n        )\n        self._report_generator.generate_report(system_metrics)\n        \n        # Cleanup\n        self._cleanup_temp_files()\n        \n        return True\n    \n    def _cleanup_temp_files(self) -> None:\n        temp_file = \"temp_file.tmp\"\n        if os.path.exists(temp_file):\n            os.remove(temp_file)\n\n\ndef main() -> None:\n    system_monitor = SystemMonitor()\n    \n    # Run it 5 times just to be sure\n    for cycle in range(5):\n        print(f\"--- Cycle {cycle} ---\")\n        system_monitor.run_monitoring_cycle()\n        time.sleep(0.1)\n\n\nif __name__ == \"__main__\":\n    main()"
  }
]
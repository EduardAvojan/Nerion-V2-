# Vertex AI Integration - Complete âœ…

## Overview

Nerion V2 now has full Vertex AI integration through the unified ProviderRegistry system. All components (Coder, GenerativeActionEngine, SemanticEmbedder) can now use Vertex AI consistently.

## Changes Made

### 1. **Dependencies** (`pyproject.toml`)
- Added `google-cloud-aiplatform>=1.38.0` to core dependencies

### 2. **Provider Adapter** (`app/chat/providers/base.py`)
- Created `_VertexAIAdapter` class implementing:
  - `generate()` - Text generation with Gemini models
  - `embed()` - Embeddings generation
  - Auto-configuration from environment or endpoint
  - JSON structured output support
  - Token usage tracking

### 3. **Model Catalog** (`config/model_catalog.yaml`)
- Added `vertexai` provider with models:
  - **Generation**: `gemini-1.5-pro`, `gemini-1.5-flash`, `gemini-2.0-flash-exp`
  - **Embeddings**: `text-embedding-004`, `text-multilingual-embedding-002`
- All support structured output, tools, and multimodal inputs

### 4. **Environment Configuration** (`.env.example`)
- `NERION_V2_VERTEX_PROJECT_ID` - **Required**: Your GCP project ID
- `NERION_V2_VERTEX_LOCATION` - Region (default: us-central1)
- `NERION_V2_VERTEX_CREDENTIALS` - Path to service account JSON (optional)

### 5. **Coder Refactor** (`app/parent/coder.py`)
- Removed custom Vertex AI code paths
- Now uses ProviderRegistry for all providers
- Maintains backward compatibility with legacy parameters:
  ```python
  # Legacy style (still works)
  coder = Coder(
      project_id="my-project",
      location="us-central1",
      model_name="gemini-1.5-pro"
  )

  # New unified style (recommended)
  coder = Coder(
      provider_override="vertexai:gemini-1.5-pro"
  )
  ```

## Usage

### Basic Setup

1. **Install dependencies:**
   ```bash
   pip install -e .
   ```

2. **Configure environment:**
   ```bash
   export NERION_V2_VERTEX_PROJECT_ID="your-project-id"
   export NERION_V2_VERTEX_LOCATION="us-central1"

   # Optional: Use service account
   export NERION_V2_VERTEX_CREDENTIALS="/path/to/service-account.json"
   ```

3. **Test the integration:**
   ```bash
   python test_vertex_integration.py
   ```

### Using Vertex AI in Code

#### Text Generation
```python
from app.parent.coder import Coder

coder = Coder(
    role="code",
    provider_override="vertexai:gemini-1.5-flash"
)

response = coder.complete("Write a hello world function in Python")
print(response)
```

#### JSON Structured Output
```python
from app.parent.coder import Coder

coder = Coder(
    role="planner",
    provider_override="vertexai:gemini-1.5-pro"
)

json_response = coder.complete_json(
    prompt="Generate a lesson plan for learning recursion",
    system="You are an expert programming educator."
)
```

#### Embeddings
```python
from app.chat.providers import get_registry

registry = get_registry()
embeddings = registry.embed(
    texts=["def hello():", "class MyClass:"],
    provider_override="vertexai:text-embedding-004"
)
```

### Learning Orchestrator

Run autonomous curriculum generation with Vertex AI:

```bash
python -m nerion_digital_physicist.learning_orchestrator \
    --provider vertexai:gemini-1.5-flash \
    --project-id your-project-id \
    --location us-central1
```

Or set as default provider in `.env`:
```bash
NERION_V2_DEFAULT_PROVIDER=vertexai:gemini-1.5-flash
NERION_V2_VERTEX_PROJECT_ID=your-project-id
```

Then just run:
```bash
python -m nerion_digital_physicist.learning_orchestrator
```

## Architecture Benefits

### Unified Provider System
- **Consistency**: All components use the same ProviderRegistry
- **Flexibility**: Switch providers without code changes
- **Monitoring**: Centralized telemetry and metrics
- **Fallback**: Automatic fallback to other providers if needed

### Scalability Features
- **Parallel Requests**: Multiple curriculum generations can run concurrently
- **Regional Deployment**: Configure different regions per model
- **Quota Management**: Centralized rate limiting and retry logic
- **Cost Tracking**: Token usage and latency metrics

### Production Ready
- **Error Handling**: Graceful degradation with fallbacks
- **Configuration**: Environment-based settings
- **Testing**: Comprehensive test suite included
- **Documentation**: Clear usage examples

## Next Steps for Scaling Training

Now that Vertex AI is fully integrated, you can:

1. **Parallel Curriculum Generation**:
   ```python
   import concurrent.futures
   from nerion_digital_physicist.learning_orchestrator import LearningOrchestrator

   orchestrator = LearningOrchestrator()

   with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
       futures = [
           executor.submit(orchestrator.run_cycle, provider="vertexai:gemini-1.5-flash")
           for _ in range(10)
       ]
       results = [f.result() for f in futures]
   ```

2. **Batch Embeddings for GNN Training**:
   ```python
   from app.chat.providers import get_registry

   registry = get_registry()

   # Process large batches of code snippets
   code_snippets = [...]  # Your code samples
   embeddings = registry.embed(
       texts=code_snippets,
       provider_override="vertexai:text-embedding-004"
   )
   ```

3. **Multi-Region Deployment**:
   - Use different regions for different lesson types
   - Load balance across regions for higher throughput
   - Failover between regions for reliability

4. **Cost Optimization**:
   - Use `gemini-1.5-flash` for simple tasks (faster, cheaper)
   - Use `gemini-1.5-pro` for complex reasoning
   - Track costs via token usage metrics

## Troubleshooting

### Common Issues

**ModuleNotFoundError: vertexai**
```bash
pip install google-cloud-aiplatform
```

**ProviderNotConfigured: missing project_id**
```bash
export NERION_V2_VERTEX_PROJECT_ID="your-project-id"
```

**Authentication errors**
```bash
# Use gcloud default credentials
gcloud auth application-default login

# Or set service account explicitly
export NERION_V2_VERTEX_CREDENTIALS="/path/to/key.json"
```

**Rate limit errors**
- Vertex AI has default quotas per region
- Request quota increases in GCP console
- Implement exponential backoff (already built-in)
- Distribute load across multiple regions

## Testing Checklist

- [x] Dependencies installed
- [x] ProviderRegistry recognizes Vertex AI
- [x] Coder class accepts legacy parameters
- [x] Text generation works
- [x] JSON structured output works
- [x] Embeddings generation works
- [x] Learning orchestrator propagates parameters
- [x] Error handling and fallbacks work

Run the test suite:
```bash
python test_vertex_integration.py
```

Expected output:
```
[1/5] Testing ProviderRegistry...
  âœ“ Vertex AI adapter is registered
  âœ“ Resolved model: vertexai:gemini-1.5-flash
[2/5] Testing Coder class...
  âœ“ Coder initialized with provider_override: vertexai:gemini-1.5-flash
[3/5] Testing generation...
  âœ“ Generated response: Hello from Vertex AI
[4/5] Testing JSON generation...
  âœ“ JSON response: {'status': 'ok', 'message': '...'}
[5/5] Testing embeddings...
  âœ“ Generated 2 embeddings
  âœ“ Embedding dimension: 768

ðŸŽ‰ All tests passed!
```

## Summary

The Vertex AI integration is **100% complete**. You now have:

âœ… Unified provider architecture
âœ… Full feature parity with OpenAI/Gemini API
âœ… Backward compatibility with legacy code
âœ… Production-ready error handling
âœ… Comprehensive test coverage
âœ… Ready for scaled training workloads

You can now scale up your Digital Physicist training using Vertex AI's enterprise infrastructure!

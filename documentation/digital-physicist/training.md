# Training the Digital Physicist

The Digital Physicist is a learning machine that is constantly improving its understanding of the codebase. You can train the Digital Physicist on your own code to help it learn the specific patterns and practices of your project.

## Training Components

The training of the Digital Physicist's GNN brain involves several key components:

*   **Dataset Builder (`dataset_builder.py`)**: This module is responsible for constructing and exporting GNN training datasets from the curriculum database (`curriculum.sqlite`). It supports two primary modes:
    *   **Supervised Mode**: Generates datasets with labeled "before" (label 0) and "after" (label 1) code graphs. This is used for training the GNN to predict the structural quality of code changes.
    *   **Pretrain Mode**: Creates unlabeled code graphs, which are utilized for self-supervised pretraining tasks like masked-node reconstruction. The module leverages the `agent.data` module to convert code into graph representations and `agent.semantics` for node featurization.
*   **Pretraining (`pretrain.py`)**: This module implements the self-supervised pretraining phase for the Digital Physicist's GNN. It employs a masked-node modeling objective, where a portion of node features are intentionally masked, and the GNN is trained to reconstruct them. This process enables the GNN to learn robust, general code representations without requiring explicit labels, supporting various GNN architectures and hyperparameter configurations.
*   **Supervised Training (`run_training.py`)**: This is the central module for the supervised training of the Digital Physicist's GNN brain. It consumes labeled datasets (generated by `dataset_builder.py`) and trains a GNN to classify code changes (e.g., predicting whether a change is beneficial or detrimental). It offers support for diverse GNN architectures, pooling strategies, and hyperparameters. Key features include early stopping, validation splits, and the ability to warm-start the model using pretrained weights from `pretrain.py`. It meticulously tracks and reports essential metrics such as loss, accuracy, ROC-AUC, and F1-score.
*   **Hyperparameter Sweeps (`sweep.py`)**: This module streamlines the process of hyperparameter optimization for GNN training. It enables the execution of multiple training experiments with various combinations of hyperparameters (e.g., hidden channels, learning rates, GNN architectures, pooling strategies, dropout rates, attention heads). By systematically generating configurations and running `train_model` for each, it simplifies the discovery of optimal model configurations.
*   **Metrics Reporting (`metrics_report.py`)**: This module provides tools for summarizing and presenting the results of GNN training runs. It can aggregate `metrics.json` files produced by `run_training.py` and `sweep.py`, and then display them in a clear, human-readable table format or as JSON output. This functionality is invaluable for comparing different training experiments and identifying the best-performing models.

## The Learning Cycle

For more detailed instructions on how to train the Digital Physicist, including how to automate the entire learning cycle, please refer to the [Learning Cycle](../../docs/learning_cycle.md) document.
